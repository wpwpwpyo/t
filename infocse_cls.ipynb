{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed46e50-9b24-4712-adaa-1ff7008c5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import faiss\n",
    "import time\n",
    "import re\n",
    "from nltk import ngrams\n",
    "import json\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import math\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9bb33-0b6f-4647-bdfb-aa76088890e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for infocse embedding\n",
    "class word2embeddings_faiss():\n",
    "    def __init__(self,all_word2embeddings):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"/mmu_nlp/wuxing/wangpeng/N3DA/InfoCSE-bert-base\")\n",
    "        self.model = AutoModel.from_pretrained(\"/mmu_nlp/wuxing/wangpeng/N3DA/InfoCSE-bert-base\").to('cuda')\n",
    "        \n",
    "        self.word2idx,self.idx2word,self.idx2embeddings,self.gpu_index=self.build_faiss(all_word2embeddings)\n",
    "\n",
    "        self.word2idx_keys=list(self.word2idx.keys())\n",
    "        self.word2idx_keys.sort()\n",
    "        \n",
    "    def build_faiss(self,all_word2embeddings):\n",
    "        print('Building idx2embeddings...')\n",
    "        word2idx={}\n",
    "        idx2word={}\n",
    "        idx2embeddings=[]\n",
    "        i=0\n",
    "        for w2e in tqdm(all_word2embeddings):\n",
    "            for word,emb in w2e.items():\n",
    "                word2idx[word]=i\n",
    "                idx2word[i]=word\n",
    "                idx2embeddings.append(emb)\n",
    "                i+=1\n",
    "\n",
    "        assert len(word2idx)==len(idx2word)==len(idx2embeddings)\n",
    "                \n",
    "        print('Building faiss...')\n",
    "        start_time=time.time()\n",
    "        idx2embeddings=np.array(idx2embeddings).astype('float32')\n",
    "        d=idx2embeddings.shape[1]\n",
    "        nlist=60000 #4*sqrt(total_num)~16*sqrt(total_num)\n",
    "        m=4 #divide embeddings into m parts->d/m == int and 2^n_bits/m == int\n",
    "        n_bits=8 #the size of each part of embeddings,8 means 256 cluster center per part / must <=8\n",
    "        ngpus = faiss.get_num_gpus()\n",
    "        print(\"number of GPUs:\", ngpus)\n",
    "        cpu_index = faiss.IndexFlatL2(d)\n",
    "        cpu_index = faiss.IndexIVFPQ(cpu_index,d,nlist,m,n_bits)\n",
    "        end_time=time.time()\n",
    "        print(f'time: {end_time-start_time}')\n",
    "\n",
    "        start_time=time.time()\n",
    "        gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)   # build the index\n",
    "        print(\"Adding to the gpu_index......\")   \n",
    "        gpu_index.train(idx2embeddings)\n",
    "        gpu_index.add(idx2embeddings)  \n",
    "        gpu_index.nprobe=int(nlist*0.1) #must int type\n",
    "        print(gpu_index.ntotal)\n",
    "        end_time=time.time()\n",
    "        print(f'time: {end_time-start_time}')\n",
    "\n",
    "        return word2idx,idx2word,idx2embeddings,gpu_index\n",
    "\n",
    "    def binarySearch (self,l, r, x, arr): \n",
    "        if r >= l: \n",
    "            mid = int(l + (r - l)/2)\n",
    "            if arr[mid] == x: \n",
    "                return mid \n",
    "            elif arr[mid] > x: \n",
    "                return self.binarySearch(l, mid-1, x,arr) \n",
    "            else: \n",
    "                return self.binarySearch(mid+1, r, x,arr) \n",
    "    \n",
    "        else: \n",
    "            return -1\n",
    "    \n",
    "    def get_syn_embedding(self,word=None,k=6):\n",
    "        if self.binarySearch(0, len(self.word2idx_keys)-1, word, self.word2idx_keys)>=0:\n",
    "            word_idx=self.word2idx[word]\n",
    "            search=np.array([self.idx2embeddings[word_idx]])\n",
    "        else:\n",
    "            word_idx=-1\n",
    "            inputs = self.tokenizer(word, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "            with torch.no_grad():\n",
    "                search = np.asarray(self.model(**inputs, output_hidden_states=True, return_dict=True).pooler_output.cpu().detach(),\"float32\")\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.binarySearch(0, len(self.word2idx_keys)-1, word, self.word2idx_keys)>=0:\n",
    "            word_idx=self.word2idx[word]\n",
    "            search=np.array([self.idx2embeddings[word_idx]])\n",
    "        else:\n",
    "            return []\n",
    "        \"\"\"\n",
    "\n",
    "        D, I = self.gpu_index.search(search, k)\n",
    "        if I[0][0]==word_idx:\n",
    "            syn_idx=I[0][1:]\n",
    "        else:\n",
    "            syn_idx=I[0][:-1]\n",
    "        syn_words=[]\n",
    "        if '<s>' in word:\n",
    "            for idx in syn_idx:\n",
    "                if idx >=0 and '<s>' in self.idx2word[idx]:\n",
    "                    syn_words.append(self.idx2word[idx])\n",
    "        elif '</s>' in word:\n",
    "            for idx in syn_idx:\n",
    "                if idx >=0 and '</s>' in self.idx2word[idx]:\n",
    "                    syn_words.append(self.idx2word[idx])\n",
    "        else:\n",
    "            for idx in syn_idx:\n",
    "                if idx >=0:\n",
    "                    syn_words.append(self.idx2word[idx])\n",
    "        return syn_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3501c9-7171-4909-b103-405fdc8e3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=glob.glob('/mmu_nlp/wuxing/wangpeng/N3DA/transformers-data-augmentation/src/bert_aug/infocse_cls/*.txt')\n",
    "\n",
    "def read_files(file):\n",
    "    word2embeddings={}\n",
    "    with open(file, 'r') as f:\n",
    "        for line in tqdm(f, total=sum(1 for _ in open(file, 'r')), desc=f\"Dealing with {file}\"):\n",
    "            values = line.strip().split('\\t')\n",
    "            word = values[0]\n",
    "            embedding = values[1].split()\n",
    "            word2embeddings[word]=embedding\n",
    "    return word2embeddings\n",
    "\n",
    "# 创建进程池\n",
    "pool = multiprocessing.Pool(processes=30)\n",
    "\n",
    "# 使用map_async函数并行读取文件\n",
    "result = pool.map_async(read_files, files)\n",
    "\n",
    "# 等待所有进程结束\n",
    "result.wait()\n",
    "\n",
    "# 所有进程结束后，result.get()返回结果\n",
    "all_word2embeddings=result.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a7b58-a191-4fe7-9efb-a4d94f3f15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embeddings_faiss=word2embeddings_faiss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2ff48-a6dc-4ed8-aaf7-c2fec11636b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_chars(line):\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"’\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \")  # replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +', ' ', clean_line)  # delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc27359-00b7-4aa1-be25-c3d63b06e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_syn_replacement_Sa(ngram,n,word2embeddings_faiss):\n",
    "    new_ngram=ngram.copy()\n",
    "    is_one_gram=type(ngram[0])!=tuple\n",
    "\n",
    "    random.shuffle(new_ngram)\n",
    "    replace_list=[]\n",
    "    num_replaced = 0\n",
    "    for random_word in new_ngram:\n",
    "        if is_one_gram:\n",
    "            synonyms = word2embeddings_faiss.get_syn_embedding(word=random_word)\n",
    "        else:\n",
    "            synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(random_word))\n",
    "        if synonyms!=[]:\n",
    "            synonym = random.choice(synonyms)\n",
    "            synonym=tuple(synonym.split())\n",
    "            replace_list.append((' '.join(random_word),' '.join(synonym)))\n",
    "            # print(\"replaced\", random_word, \"with\", synonym)\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:  # only replace up to n words\n",
    "            break\n",
    "    return replace_list,num_replaced\n",
    "\n",
    "def synonym_replacement_Sa(words, n,word2embeddings_faiss):\n",
    "    three_ngram=list(ngrams(words,3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))[1:-1]\n",
    "    num_replaced=0\n",
    "    replace_list,num_replaced_three=ngram_syn_replacement_Sa(three_ngram, n, word2embeddings_faiss)\n",
    "    num_replaced+=num_replaced_three\n",
    "    \n",
    "    if num_replaced_three!=0:\n",
    "        sentence=' '.join(words)\n",
    "        for old,new in replace_list:\n",
    "            old=old.strip()\n",
    "            old=' '+old+' '\n",
    "            if '<s>' in old:\n",
    "                old=old.lstrip()\n",
    "            if '</s>' in old:\n",
    "                old=old.rstrip()\n",
    "\n",
    "            new=new.strip()\n",
    "            new=' '+new+' '\n",
    "            if '<s>' in new:\n",
    "                new=new.lstrip()\n",
    "            if '</s>' in new:\n",
    "                new=new.rstrip()\n",
    "\n",
    "            old=old.replace('<s> ','').replace(' </s>','')\n",
    "            new=new.replace('<s> ','').replace(' </s>','')\n",
    "            sentence=sentence.replace(old, new)\n",
    "        words=sentence.split()\n",
    "\n",
    "    if num_replaced < n:\n",
    "        two_ngram=list(ngrams(words,2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "        replace_list,num_replaced_two=ngram_syn_replacement_Sa(two_ngram, n, word2embeddings_faiss)\n",
    "        num_replaced+=num_replaced_two\n",
    "        sentence=''\n",
    "        if num_replaced_two!=0:\n",
    "            sentence=' '.join(words)\n",
    "            for old,new in replace_list:\n",
    "                old=old.strip()\n",
    "                old=' '+old+' '\n",
    "                if '<s>' in old:\n",
    "                    old=old.lstrip()\n",
    "                if '</s>' in old:\n",
    "                    old=old.rstrip()\n",
    "\n",
    "                new=new.strip()\n",
    "                new=' '+new+' '\n",
    "                if '<s>' in new:\n",
    "                    new=new.lstrip()\n",
    "                if '</s>' in new:\n",
    "                    new=new.rstrip()\n",
    "\n",
    "                old=old.replace('<s> ','').replace(' </s>','')\n",
    "                new=new.replace('<s> ','').replace(' </s>','')\n",
    "                sentence=sentence.replace(old, new)\n",
    "            words=sentence.split()\n",
    "\n",
    "    if num_replaced < n:\n",
    "        replace_list,num_replaced_one=ngram_syn_replacement_Sa(words, n, word2embeddings_faiss)\n",
    "        num_replaced+=num_replaced_one\n",
    "        if num_replaced_one!=0:\n",
    "            sentence=' '.join(words)\n",
    "            for old,new in replace_list:\n",
    "                old=old.strip()\n",
    "                old=' '+old+' '\n",
    "                if '<s>' in old:\n",
    "                    old=old.lstrip()\n",
    "                if '</s>' in old:\n",
    "                    old=old.rstrip()\n",
    "\n",
    "                new=new.strip()\n",
    "                new=' '+new+' '\n",
    "                if '<s>' in new:\n",
    "                    new=new.lstrip()\n",
    "                if '</s>' in new:\n",
    "                    new=new.rstrip()\n",
    "\n",
    "                old=old.replace('<s> ','').replace(' </s>','')\n",
    "                new=new.replace('<s> ','').replace(' </s>','')\n",
    "                sentence=sentence.replace(old, new)\n",
    "            words=sentence.split()\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30b8ec-4481-4c92-9ed4-828d06875f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_syn_replacement_Sb(ngram,n,word2embeddings_faiss):\n",
    "    new_ngram=ngram.copy()\n",
    "\n",
    "    random.shuffle(new_ngram)\n",
    "    replace_list=[]\n",
    "    num_replaced = 0\n",
    "    for random_word in new_ngram:\n",
    "        synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(random_word))\n",
    "        if synonyms!=[]:\n",
    "            synonym = random.choice(synonyms)\n",
    "            synonym=tuple(synonym.split())\n",
    "            replace_list.append((' '.join(random_word),' '.join(synonym)))\n",
    "            # print(\"replaced\", random_word, \"with\", synonym)\n",
    "            num_replaced += 1\n",
    "        else:\n",
    "            de_gram=list(ngrams(random_word,2))\n",
    "            one_gram=[]\n",
    "            random.shuffle(de_gram)\n",
    "            for two in de_gram:\n",
    "                one_gram.extend(list(two))\n",
    "            one_gram=list(set(one_gram))\n",
    "            random.shuffle(one_gram)\n",
    "            de_gram.extend(one_gram)\n",
    "            for de in de_gram:\n",
    "                if type(de)!=tuple: #one_gram\n",
    "                    synonyms = word2embeddings_faiss.get_syn_embedding(word=de)\n",
    "                else:\n",
    "                    synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(de))\n",
    "                    if synonyms!=[]:\n",
    "                        synonym = random.choice(synonyms)\n",
    "                        synonym=tuple(synonym.split())\n",
    "                        replace_list.append((' '.join(de),' '.join(synonym)))\n",
    "                        # print(\"replaced\", random_word, \"with\", synonym)\n",
    "                        num_replaced += 1\n",
    "            \n",
    "                    if num_replaced >= n:\n",
    "                        break\n",
    "\n",
    "        if num_replaced >= n:  # only replace up to n words\n",
    "            break\n",
    "    return replace_list,num_replaced\n",
    "\n",
    "def synonym_replacement_Sb(words, n,word2embeddings_faiss):\n",
    "    three_ngram=list(ngrams(words,3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))[1:-1]\n",
    "    replace_list,num_replaced=ngram_syn_replacement_Sb(three_ngram, n, word2embeddings_faiss)\n",
    "    \n",
    "    if num_replaced!=0:\n",
    "        sentence=' '.join(words)\n",
    "        for old,new in replace_list:\n",
    "            old=old.strip()\n",
    "            old=' '+old+' '\n",
    "            if '<s>' in old:\n",
    "                old=old.lstrip()\n",
    "            if '</s>' in old:\n",
    "                old=old.rstrip()\n",
    "\n",
    "            new=new.strip()\n",
    "            new=' '+new+' '\n",
    "            if '<s>' in new:\n",
    "                new=new.lstrip()\n",
    "            if '</s>' in new:\n",
    "                new=new.rstrip()\n",
    "\n",
    "            old=old.replace('<s> ','').replace(' </s>','')\n",
    "            new=new.replace('<s> ','').replace(' </s>','')\n",
    "            sentence=sentence.replace(old, new)\n",
    "        words=sentence.split()\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fb559-61f3-47b2-822f-ca1a195e3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_syn_replacement_Sc(ngram,n,word2embeddings_faiss):\n",
    "    new_ngram=ngram.copy()\n",
    "\n",
    "    replace_list=[]\n",
    "    num_replaced = 0\n",
    "    for random_word in new_ngram:\n",
    "        if type(random_word)!=tuple:\n",
    "            synonyms = word2embeddings_faiss.get_syn_embedding(word=random_word)\n",
    "        else:\n",
    "            synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(random_word))\n",
    "        if synonyms!=[]:\n",
    "            synonym = random.choice(synonyms)\n",
    "            synonym=tuple(synonym.split())\n",
    "            replace_list.append((' '.join(random_word),' '.join(synonym)))\n",
    "            # print(\"replaced\", random_word, \"with\", synonym)\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:  # only replace up to n words\n",
    "            break\n",
    "    return replace_list,num_replaced\n",
    "\n",
    "def synonym_replacement_Sc(words, n,word2embeddings_faiss):\n",
    "    three_ngram=list(ngrams(words,3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))[1:-1]\n",
    "    two_ngram=list(ngrams(words,2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    three_ngram.extend(two_ngram)\n",
    "    three_ngram.extend(words)\n",
    "    random.shuffle(three_ngram)\n",
    "\n",
    "    replace_list,num_replaced=ngram_syn_replacement_Sc(three_ngram, n, word2embeddings_faiss)\n",
    "    \n",
    "    if num_replaced!=0:\n",
    "        sentence=' '.join(words)\n",
    "        for old,new in replace_list:\n",
    "            old=old.strip()\n",
    "            old=' '+old+' '\n",
    "            if '<s>' in old:\n",
    "                old=old.lstrip()\n",
    "            if '</s>' in old:\n",
    "                old=old.rstrip()\n",
    "\n",
    "            new=new.strip()\n",
    "            new=' '+new+' '\n",
    "            if '<s>' in new:\n",
    "                new=new.lstrip()\n",
    "            if '</s>' in new:\n",
    "                new=new.rstrip()\n",
    "\n",
    "            old=old.replace('<s> ','').replace(' </s>','')\n",
    "            new=new.replace('<s> ','').replace(' </s>','')\n",
    "            sentence=sentence.replace(old, new)\n",
    "        words=sentence.split()\n",
    "\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d09fb-0bf6-442e-86af-2fd199288d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(words, p):\n",
    "    # obviously, if there's only one word, don't delete it\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    # randomly delete words with probability p\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    # if you end up deleting all words, just return a random word\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words) - 1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    return new_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4857c11-38cd-4112-8ae6-742e35f539d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_insertion(words, n,word2embeddings_faiss,strategy):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        if strategy=='Sa':\n",
    "            add_word_Sa(new_words,word2embeddings_faiss)\n",
    "        elif strategy=='Sb':\n",
    "            add_word_Sb(new_words,word2embeddings_faiss)\n",
    "        elif strategy=='Sc':\n",
    "            add_word_Sc(new_words,word2embeddings_faiss)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def add_word_Sa(new_words,word2embeddings_faiss):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    three_ngram=list(ngrams(new_words,3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))[1:-1]\n",
    "    two_ngram=list(ngrams(new_words,2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = three_ngram[random.randint(0, len(three_ngram) - 1)]\n",
    "        synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(random_word))\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            break\n",
    "    \n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = two_ngram[random.randint(0, len(two_ngram) - 1)]\n",
    "        synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(random_word))\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            break\n",
    "    \n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
    "        synonyms = word2embeddings_faiss.get_syn_embedding(word=random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            break\n",
    "\n",
    "    if synonyms!=[]:\n",
    "        random_synonym = synonyms[0].replace('<s>','').replace('</s>','').strip()\n",
    "        random_idx = random.randint(0, len(new_words) - 1)\n",
    "        new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "def add_word_Sb(new_words,word2embeddings_faiss):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    three_ngram=list(ngrams(new_words,3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))[1:-1]\n",
    "\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = three_ngram[random.randint(0, len(three_ngram) - 1)]\n",
    "        synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(random_word))\n",
    "\n",
    "        if synonyms==[]:\n",
    "            de_gram=list(ngrams(random_word,2))\n",
    "            one_gram=[]\n",
    "            random.shuffle(de_gram)\n",
    "            for two in de_gram:\n",
    "                one_gram.extend(list(two))\n",
    "            one_gram=list(set(one_gram))\n",
    "            random.shuffle(one_gram)\n",
    "            de_gram.extend(one_gram)\n",
    "            for de in de_gram:\n",
    "                if type(de)!=tuple: #one_gram\n",
    "                    synonyms = word2embeddings_faiss.get_syn_embedding(word=de)\n",
    "                else:\n",
    "                    synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(de))\n",
    "                if synonyms!=[]:\n",
    "                    break\n",
    "\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            break\n",
    "\n",
    "    if synonyms!=[]:\n",
    "        random_synonym = synonyms[0].replace('<s>','').replace('</s>','').strip()\n",
    "        random_idx = random.randint(0, len(new_words) - 1)\n",
    "        new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "def add_word_Sc(new_words,word2embeddings_faiss):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    three_ngram=list(ngrams(new_words,3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))[1:-1]\n",
    "    two_ngram=list(ngrams(new_words,2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    three_ngram.extend(two_ngram)\n",
    "    three_ngram.extend(new_words)\n",
    "\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = three_ngram[random.randint(0, len(three_ngram) - 1)]                \n",
    "        if type(random_word)!=tuple: #one_gram\n",
    "            synonyms = word2embeddings_faiss.get_syn_embedding(word=random_word)\n",
    "        else:\n",
    "            synonyms = word2embeddings_faiss.get_syn_embedding(word=' '.join(random_word))\n",
    "\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            break\n",
    "\n",
    "    if synonyms!=[]:\n",
    "        random_synonym = synonyms[0].replace('<s>','').replace('</s>','').strip()\n",
    "        random_idx = random.randint(0, len(new_words) - 1)\n",
    "        new_words.insert(random_idx, random_synonym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9485c7-1bfa-4d98-91a7-ee3508320e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for textsmoothing\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9,word2embeddings_faiss=None, strategy='Sa'):\n",
    "    sentence = get_only_chars(sentence)\n",
    "    words = sentence.split(' ')\n",
    "    words = [word for word in words if word is not '']\n",
    "    num_words = len(words)\n",
    "\n",
    "    #change\n",
    "    if num_words==0:\n",
    "        return []\n",
    "\n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = int(num_aug / 3) + 1\n",
    "    n_sr = max(1, int(alpha_sr * num_words))\n",
    "    n_ri = max(1, int(alpha_ri * num_words))\n",
    "    n_rs = max(1, int(alpha_rs * num_words))\n",
    "\n",
    "    # sr\n",
    "    for _ in range(num_new_per_technique):\n",
    "        if strategy=='Sa':\n",
    "            a_words = synonym_replacement_Sa(words, n_sr,word2embeddings_faiss)\n",
    "        elif strategy=='Sb':\n",
    "            a_words = synonym_replacement_Sb(words, n_sr,word2embeddings_faiss)\n",
    "        elif strategy=='Sc':\n",
    "            a_words = synonym_replacement_Sc(words, n_sr,word2embeddings_faiss)\n",
    "        \n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    # ri\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_insertion(words, n_ri,word2embeddings_faiss,strategy)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "        \n",
    "    # rs\n",
    "    \"\"\"\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "    \"\"\"\n",
    "\n",
    "    # rd\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_deletion(words, p_rd)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "        \n",
    "    #change\n",
    "    #augmented_sentences = [sentence for sentence in augmented_sentences if sentence!='']\n",
    "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences if sentence!='']\n",
    "    shuffle(augmented_sentences)\n",
    "\n",
    "    \n",
    "    # trim so that we have the desired number of augmented sentences\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "        \n",
    "    # append the original sentence\n",
    "    augmented_sentences.append(sentence)\n",
    "\n",
    "    return augmented_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17eca24-20f0-4680-91cd-4de51120d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for textsmoothing\n",
    "def gen_eda(train_orig, output_file, alpha, strategy, num_aug=1, seed=0):\n",
    "    files=glob.glob(train_orig.replace('?','*'))\n",
    "    for file_name in files:\n",
    "        print('Deal with '+file_name)\n",
    "        files_idx=file_name.split('/')[-1]\n",
    "        if '_' in files_idx:\n",
    "            idx=int(files_idx.split('_')[1])\n",
    "        else:\n",
    "            idx=seed\n",
    "        random.seed(idx)\n",
    "        input_file=file_name+'/train.tsv'\n",
    "        output_file=file_name+'/n3da_aug.tsv'\n",
    "        #for debug\n",
    "        #input_file=file_name\n",
    "        #output_file=file_name+'.output'\n",
    "\n",
    "        writer = open(output_file, 'w')\n",
    "        lines = open(input_file, 'r').readlines()\n",
    "        #for i,line in enumerate(lines):\n",
    "        for line in tqdm(lines):\n",
    "            \"\"\"\n",
    "            print(i)\n",
    "            if i<51975:\n",
    "                continue\n",
    "            \"\"\"\n",
    "            label,line=line.strip().split('\\t')\n",
    "            aug_sentences = eda(line, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug, word2embeddings_faiss=word2embeddings_faiss,strategy=strategy)\n",
    "            if len(aug_sentences)==0:\n",
    "                continue\n",
    "            assert len(aug_sentences)>=2\n",
    "            for sentence in aug_sentences:\n",
    "                if sentence!='':\n",
    "                    writer.write(label + \"\\t\" + sentence + '\\n')\n",
    "        writer.close()\n",
    "    \n",
    "        print(\"generated augmented sentences with eda for \" + input_file + \" to \" + output_file + \" with num_aug=\" + str(num_aug) + \" with alpha=\" + str(alpha) + \" with seed=\" + str(idx) + \" with strategy=\" + str(strategy))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6575ee-91e3-4715-8943-269a196366ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for textsmoothing\n",
    "def main(task,is_full,num_aug,alpha,seed=0,strategy='Sa'):\n",
    "    SRC='/mmu_nlp/wuxing/wangpeng/N3DA/transformers-data-augmentation/src'\n",
    "    TASK=task\n",
    "    if is_full:\n",
    "        RAWDATADIR=SRC+f'/utils/datasets/{TASK}'\n",
    "    else:\n",
    "        RAWDATADIR=SRC+f'/utils/datasets/{TASK}/exp_?_?'\n",
    "    input_file=RAWDATADIR\n",
    "    output=None\n",
    "\n",
    "    gen_eda(input_file, output, alpha=alpha, num_aug=num_aug, seed=seed, strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bce80f-fe80-4a69-8e1e-718aaaa1b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for textsmoothing\n",
    "#def main(task,is_full,num_aug,alpha)\n",
    "if __name__ == \"__main__\":\n",
    "    alpha=0.1\n",
    "    strategy='Sa'\n",
    "    seed=0\n",
    "    num_aug=1\n",
    "    main('trec',True,num_aug,alpha,seed,strategy)\n",
    "    main('stsa',True,num_aug,alpha,seed,strategy)\n",
    "    main('snips',True,num_aug,alpha,seed,strategy)\n",
    "    main('trec',False,num_aug,alpha,seed,strategy)\n",
    "    main('stsa',False,num_aug,alpha,seed,strategy)\n",
    "    main('snips',False,num_aug,alpha,seed,strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
